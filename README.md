# ğŸ¶ CNN-Based Musical Instrument Recognition System

## ğŸ“Œ Overview

This project presents an end-to-end **Convolutional Neural Network (CNN)â€“based system** for automatic musical instrument recognition from audio samples.

Raw audio signals are transformed into **log-mel spectrograms**, treated as image representations, and classified using deep learning techniques.

The system was developed incrementally through structured milestones, covering:

- Data preprocessing  
- Baseline model development  
- Model tuning & evaluation  
- Multi-instrument detection  
- Instrument condition analysis  
- Web deployment  

---

## ğŸµ Dataset

- **NSynth Dataset (Acoustic Subset)** by Google Magenta  
- High-quality, monophonic instrument recordings  
- Audio samples converted to log-mel spectrograms  
- 8 instrument classes used:

  - Brass  
  - Flute  
  - Guitar  
  - Keyboard  
  - Mallet  
  - Reed  
  - String  
  - Vocal  

---

## ğŸ— Project Structure

```
Scripts/                  â†’ Audio preprocessing and data pipeline
Sample_Spectrograms/      â†’ Spectrogram visual validation
Milestone2/               â†’ Baseline CNN training
Milestone3/               â†’ Tuned CNN model (v3)
Milestone4/               â†’ Deployment + Multi-instrument + Condition analysis
Notebooks/                â†’ Initial experimental notebooks
README.md                 â†’ Project documentation
```

---

## ğŸš€ Milestones Summary

### ğŸ”¹ Milestone 1 â€“ Data Preprocessing

- Audio standardization (22,050 Hz, mono conversion)
- Fixed-duration trimming/padding
- Log-mel spectrogram extraction
- dB scale conversion
- Normalization to [0,1]
- Clean NumPy pipeline (`X.npy`, `y.npy`)
- Spectrogram validation via visual inspection

---

### ğŸ”¹ Milestone 2 â€“ Baseline CNN Model

- Basic CNN architecture
- Input shape: 128 Ã— 128 Ã— 1
- Validation accuracy â‰ˆ **78%**
- Confusion matrix analysis performed
- Identified class-level confusion patterns

---

### ğŸ”¹ Milestone 3 â€“ Model Evaluation & Tuning

- Batch Normalization experiment (discarded after degradation)
- Deeper CNN architecture introduced
- Improved feature extraction capacity
- Validation accuracy improved to **92â€“93%**
- Reduced confusion among similar instruments
- Final model selected: `instrunet_model_v3.keras`

---

### ğŸ”¹ Milestone 4 â€“ Deployment & Extended Analysis

This milestone expands the system beyond basic classification.

#### ğŸ¼ Multi-Instrument Detection
- Segment-wise audio splitting
- Probability aggregation across segments
- Instrument intensity visualization
- Timeline representation

#### ğŸ§± Instrument Condition Analysis
Based on harmonic fingerprint extraction:

- Harmonic-to-Noise Ratio (HNR)
- Spectral Flatness
- Decay Variance

Instrument classified as:
- Healthy  
- Moderately Aged  
- Broken / Noisy  

#### ğŸŒ Web Deployment
- Built using **Streamlit**
- Interactive audio upload interface
- Real-time prediction display
- JSON & detailed academic PDF export

ğŸ”— **Live Application:**  
https://instrunet-ai-g5ra8bxquz2djj8qnbpjnz.streamlit.app/

---

## ğŸ§  Technical Stack

- Python  
- TensorFlow / Keras  
- Librosa  
- NumPy  
- Matplotlib  
- Streamlit  
- FPDF  

---

## ğŸ“Š Model Information

### Instrument Classification Model
- CNN trained on mel-spectrogram images
- Input size: 128 Ã— 128 Ã— 1
- Validation Accuracy â‰ˆ 92â€“93%
- Softmax-based confidence output

### Condition Classification
- Trained using augmented degraded audio
- Three classes: Healthy / Aged / Broken
- Harmonic analysis integrated into deployment pipeline

---

## â–¶ï¸ Running Locally

Navigate to Milestone4:

```
cd Milestone4
pip install -r requirements.txt
streamlit run app.py
```

---

## ğŸ“Œ Notes

- Trained `.keras` model files are included in the Milestone4 folder for deployment.
- Dataset audio files are not included due to size constraints.
- Performance plots and confusion matrices are available in milestone folders.

---

## ğŸ‘¨â€ğŸ’» Author

Sudhan-io  
CNN-Based Musical Instrument Recognition & Analysis System  